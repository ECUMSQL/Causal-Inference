- [模型与算法](#模型与算法)
  - [1. 分类回归模型](#1-分类回归模型)
    - [1.1 线性模型](#11-线性模型)
      - [1.1.1 线性回归模型](#111-线性回归模型)
      - [1.1.2 岭回归模型](#112-岭回归模型)
      - [1.1.3 Lasso回归模型](#113-lasso回归模型)
    - [1.2 常用二分类模型](#12-常用二分类模型)
      - [1.2.1逻辑回归模型](#121逻辑回归模型)
      - [1.2.2决策树CART决策树](#122决策树cart决策树)
      - [1.2.3决策树ID3算法](#123决策树id3算法)
      - [1.2.4决策C4.5算法](#124决策c45算法)
    - [1.3 集成学习模型](#13-集成学习模型)
      - [1.3.1 随机森林](#131-随机森林)
      - [1.3.2 Adaboost](#132-adaboost)
      - [1.3.3 GBDT集成算法](#133-gbdt集成算法)
    - [1.4 其他模型](#14-其他模型)
      - [1.4.1 SVM支持向量机](#141-svm支持向量机)
      - [1.4.2 感知机二分类模型](#142-感知机二分类模型)
      - [1.4.3 朴素贝叶斯模型](#143-朴素贝叶斯模型)
  - [2. 过拟合和预防过拟合](#2-过拟合和预防过拟合)
    - [2.1 过拟合](#21-过拟合)
    - [2.2 预防过拟合](#22-预防过拟合)
  - [3. 模型评估](#3-模型评估)
    - [3.1 信息熵](#31-信息熵)
    - [3.2 评估指标](#32-评估指标)
- [降维与聚类](#降维与聚类)
  - [1.聚类算法](#1聚类算法)
    - [1.1 K-means聚类](#11-k-means聚类)
    - [1.2 DBSCAN聚类](#12-dbscan聚类)
    - [1.3 层次聚类](#13-层次聚类)
    - [1.4 Kohonen聚类](#14-kohonen聚类)
    - [1.5.实现算法](#15实现算法)
  - [2.降维算法](#2降维算法)
    - [2.1 SVD降维](#21-svd降维)
    - [2.2 PCA主成分分析](#22-pca主成分分析)
    - [2.3 LDA线性判别分析](#23-lda线性判别分析)
    - [2.4 FA因子分析](#24-fa因子分析)
    - [2.5 Fisher-LDA算法](#25-fisher-lda算法)
- [训练与求解](#训练与求解)
  - [1.求解算法](#1求解算法)
    - [1.1 最小二乘法](#11-最小二乘法)
    - [1.2 最小二乘和QA分解](#12-最小二乘和qa分解)
    - [1.3 广义特征值分解](#13-广义特征值分解)
  - [2.优化算法](#2优化算法)
    - [2.1 梯度下降法](#21-梯度下降法)
    - [2.2 牛顿法](#22-牛顿法)
    - [2.3 动量梯度下降法](#23-动量梯度下降法)
    - [2.4 自适应梯度下降法](#24-自适应梯度下降法)
    - [2.5 坐标梯度下降法](#25-坐标梯度下降法)

# <div align="center">模型与算法</div>
[内容来源](https://www.bbbdata.com/text/597)
## 1. 分类回归模型

### 1.1 线性模型

#### 1.1.1 线性回归模型 

具体内容看stata内容和笔记

#### 1.1.2 岭回归模型 

具体内容看stata内容和笔记

#### 1.1.3 Lasso回归模型 

具体内容看stata内容和笔记

### 1.2 常用二分类模型

#### 1.2.1逻辑回归模型

逻辑回归模型主要食用于二分类问题，其采用回归与概率的关系，这里的概率是指被处理的概率，最终形成一个logistic函数。然后采用极大似然函数，进行梯度下降法进行求解。具体可看stata order.（其中Ir为下降系数，自己定）***这个和回归不同，回归系数是e次方***

<div align="center">
    <img src="梯度下降法.png" width="70%">
    <p style="font-size:18px;">梯度下降法</p>
</div>

#### 1.2.2决策树CART决策树

决策树模型（Decision Tree）是机器学习中最常用的模型之一，它可用于分类，也可用于回归。是一棵二叉树，且只支持数值变量。
**CART决策树在构建时，每次选择一个变量与切割点将样本一分为二，得到左右两个节点左节点与右节点作为子节点继续各自生长、分裂，直到满足条件则停止生长，完成构建。**
决策树构建的目标就是构建出一棵树，使得历史样本在决策树上分类尽量准确
***步骤***：

1. 初始化一个根节点                                  
2. 对非叶子节点进行分枝：
    对非叶子节点选择一个变量与一个分割值，将样本一分为二，得到左右两个节点
    并判断左右两个节点是否满足成为叶子节点的条件（**基尼系数混乱程度**），如果满足，则标上节点的预测值
3. 重复2，直到所有节点都成为叶子节点
  
***问题与解答***：
1. ***分裂时如何选择变量与分割值：***
历遍所有的【变量-分割值】组合，哪种分割最优，就选择哪种【变量-分割值】
分类树判断【变量-分割值】的分割质量时的评估函数为：基尼函数，信息增益(熵)函数
回归树判断【变量-分割值】的分割质量时的评估函数为：平方差函数
2. ***叶子节点的预测值如何确定：***
对于分类树：叶子节点上的样本，哪个类别最多，就作为叶子节点的预测类别
对于回归树：取叶子节点上样本y值的平均值，作为叶子节点的预测值

***CART决策树节点分支的评估函数：***

1. ***分类树：基尼系数***--越低越好，前后比较
   GINI函数代表分裂后，在左(或右)节点随机抽取两个样本，它们类别不同的概率Gini函数的表达式如下：
    $$Gini(t) = (\frac{N_L}{N}*[1 - \sum_{i=1}^{k}p_{ileft}^2]+\frac{N_R}{N}*[1 - \sum_{i=1}^{k}p_{iright}^2])$$
2. ***信息增益函数***--越低越好，前后比较
   信息增益函数代表分裂后，在左(或右)节点随机抽取两个样本，它们类别不同的概率信息增益函数的表达式如下：
    $$Gain(t) = -(\frac{N_L}{N}*\sum_{i=1}^{k}p_{ileft}log_2(p_{ileft})+\frac{N_R}{N}*\sum_{i=1}^{k}p_{iright}log_2(p_{iright}))$$
3. ***回归树：平方差函数***
   平方差函数代表分裂后，在左(或右)节点随机抽取两个样本，它们y值的差异平方差函数的表达式如下：
    $$G(t) = \sum_{i=1}^{N_L}(y_i - \bar{y}_L)^2+\sum_{i=1}^{N_R}(y_i - \bar{y}_R)^2$$

***CART决策树的剪枝策略：***

1. **预剪枝**：在sklearn中就是调参数
2. **后剪枝**：一般使用CCP函数
   $$L = \sum_{t=1}^{T}\frac{N_i}{N}L_i + \alpha|T|$$
  在sklearn中，criterion设为entropy时，Li是第i个叶子的熵
  criterion设为GINI时，则是第i个叶子的GINI系数
  其中的参数为复杂度，用于惩罚子节点个数T。***因此不同的惩罚系数和代价与树的复杂度之间权衡。***

```python
# -*- coding: utf-8 -*-
from sklearn.datasets import load_iris
from sklearn import tree
import numpy as np
#----------------数据准备----------------------------
iris = load_iris()                                                    # 加载数据
X = iris.data                                                         # 用于训练的X
y = iris.target                                                       # 用于训练的y
#---------------模型训练--------------------------------- 
clf = tree.DecisionTreeClassifier(min_samples_split=10,ccp_alpha=0)   # 初始化决策树模型,这里设置min_samples_split就是一种预剪枝策略    
clf = clf.fit(X, y)                                                   # 训练决策树
pruning_path = clf.cost_complexity_pruning_path(X, y)                 # 计算CCP路径
#-------打印结果---------------------------    
print("\n====CCP路径=================")                               # 打印CCP路径
print("ccp_alphas:",pruning_path['ccp_alphas'])                       # 打印CCP路径中的alpha
print("impurities:",pruning_path['impurities'])                       # 打印CCP路径alpha对应的不纯度
```

#### 1.2.3决策树ID3算法

ID3主要使用**信息增益**来进行节点判定的选择。
其主要流程为：
1. 计算各个变量的信息增益,确定本次由哪个变量分支           
2. 对变量分支后,确定哪些节点是叶子节点，                        
  哪些需要再分，需要再分的就继续由剩余变量分支. 
3. 如果所有节点都不需分支,或所有变量已分完,则停止算法. 

***缺点：***
ID3算法的缺点如下：
  1. 变量偏好多枚举值
  2. ID3容易过拟合
  3. ID3不支持连续变量
  4. 不支持数据有缺失值

#### 1.2.4决策C4.5算法

C4.5算法是ID3算法的改进版，主要是解决ID3算法的缺点。
***纠正：***
  1. 变量偏好多枚举值--变量偏好纠正：使用信息增益比
  2. ID3容易过拟合--C4.5在ID3的基础上，加入了剪枝，来处理过拟合的问题。
  3. ID3不支持连续变量--在变量相邻值中间切割（n-1个割点，每个割点将变量割成两类
  4. 不支持数据有缺失值--最好忽略

### 1.3 集成学习模型
#### 1.3.1 随机森林

<div align="center">
    <img src="Bagging and Boosting.png" width="70%">
    <p style="font-size:18px;">Bagging and Boosting</p>
</div>

随机森林(Random Forest)是bagging集成算法的一种实现，它集成多棵Cart决策树来共同决策。（Bagging指得是一种涉及在数据的随机子集上独立训练多个模型，并通过投票或平均聚合他们的预测）

随机森是通过随机抽取**不同样本**、不同的变量来训练出**多棵弱决策树**的，所以称为随机森林
随机森林的模型表达式为：
$$f(x) = \frac{1}{N}\sum_{i=1}^{N}f_i(x)$$
其中$f_i(x)$为第i棵树的预测值
***随机森林是如何Bagging:***
1. **随机样本**：使用boostrap抽样来使样本差异化，令每次训练的模型侧重点不一样 
 boostrap抽样指的是放回式抽样n次(n是整体样本个数)     
2. **随机变量：** 每次只随机抽取部分变量来训练Cart决策树，削弱决策树的拟合能力这样既可以使每棵树使用的变量不一样，同时又能限制树的深度

<div align="center">
    <img src="随机森林.png" width="70%">
    <p style="font-size:18px;">随机森林</p>
</div>

***随机森林的模型评估***

由于bootstrap抽样，每个模型进行训练的数据都不是全部数据，而是漏出了大约33%的数据未使用，这部分数据可以用来评估模型的性能，这部分数据称为OOB(out of bag)数据。（袋外数据）
***用没有训练该数据的模型进行训练。最后的袋外得分就是对于所有模型的袋外预测的准确性的加权平均***

```python
"""
本代表用于展示如何使用sklearn实现随机森林模型
代码来自:《老饼讲解-机器学习》 www.bbbdata.com
"""
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
import numpy as np

np.random.seed(888)
#----------加载数据---------------                                          
iris = load_iris()                                                           # 加载iris数据
X    = iris.data                                                             # 样本的X
y    = iris.target                                                           # 样本的y

#------模型训练与预测---------------
clf = RandomForestClassifier(oob_score=True,max_features=2,n_estimators=100) # 初始化随机森林模型
clf.fit(X, y)                                                                # 训练随机森林模型
pred_prob = clf.predict_proba(X)                                             # 预测样本的概率
pred_c    = clf.predict(X)                                                   # 预测样本的类别
preds     = iris.target_names[pred_c]

#-------打印结果 ---------------
print("\n----前5个样本的真实类别:----") 
print(iris.target[0:5])
print("\n----前5个样本的预测结果:----") 
print(pred_c[0:5])
print("\n----袋外准确率oob_score:----") 
print(clf.oob_score_)
print("\n----特征得分:----") 
print(clf.feature_importances_)
```

#### 1.3.2 Adaboost

AdaBoost以Boosting方式集成多个弱二分类模型来产生泛化能力好、预测精度高的**强二分类模型**，其模型为：
$$f(x) = \sum_{i=1}^{N}\alpha_if_i(x)$$ 其中$\alpha$为决策器的权重，$f_i(x)$为第i个决策器的预测值，每个$f_i(x)$输出为（-1，1）
以Cart分类树作为决策器，此时，则称为 **AdaBoost提升树**
***总失函数为：***
$$L = \sum_{i=1}^{N} e^{(-y_if(x_i))}$$损失函数用的是指数损失函数
***第k个决策器的损失函数为：***
$$L_k = \sum_{i=1}^{N} e^{(-y_if_k(x_i))}$$其中k为前k个决策器

***模型训练：*** Adaboost采用前向分步算法训练，即在已有决策器的基础上逐个添加新的决策器，每个决策器都力求损失函数最小化，直到新增决策器无法降低损失函数则停止训练。

<div align="center">
    <img src="adaboost分布.png" width="80%">
    <p style="font-size:18px;">adaboost分布</p>
</div>

在训练时，使用参数和模型分开训练的方式





#### 1.3.3 GBDT集成算法

### 1.4 其他模型
#### 1.4.1 SVM支持向量机
#### 1.4.2 感知机二分类模型
#### 1.4.3 朴素贝叶斯模型

## 2. 过拟合和预防过拟合
### 2.1 过拟合
### 2.2 预防过拟合

## 3. 模型评估
### 3.1 信息熵
### 3.2 评估指标

# <div align="center">降维与聚类</div>
## 1.聚类算法
### 1.1 K-means聚类
### 1.2 DBSCAN聚类
### 1.3 层次聚类
### 1.4 Kohonen聚类
### 1.5.实现算法

## 2.降维算法
### 2.1 SVD降维
### 2.2 PCA主成分分析
### 2.3 LDA线性判别分析
### 2.4 FA因子分析
### 2.5 Fisher-LDA算法

# <div align="center">训练与求解</div>
## 1.求解算法
### 1.1 最小二乘法
### 1.2 最小二乘和QA分解
### 1.3 广义特征值分解

## 2.优化算法
### 2.1 梯度下降法
### 2.2 牛顿法
### 2.3 动量梯度下降法
### 2.4 自适应梯度下降法
### 2.5 坐标梯度下降法